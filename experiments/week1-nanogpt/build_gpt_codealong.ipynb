{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GPT from Scratch - Code Along with Karpathy\n",
    "\n",
    "**Week 1, Day 2 Activity**\n",
    "\n",
    "This notebook is for coding along with Andrej Karpathy's \"Let's build GPT: from scratch, in code, spelled out\" video.\n",
    "\n",
    "**Video Link:** https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "\n",
    "## Goal\n",
    "Build a character-level language model from scratch to deeply understand:\n",
    "- Tokenization and data preparation\n",
    "- Self-attention mechanisms\n",
    "- Multi-head attention\n",
    "- Transformer blocks\n",
    "- Full GPT architecture\n",
    "- Training and text generation\n",
    "\n",
    "## Notes\n",
    "This is exploratory/messy code. I'll extract clean implementations into `my_gpt.py` on Days 3-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare dataset\n",
    "# Implement character-level tokenization\n",
    "# Create train/val splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download the Tinyshakespear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-11 18:01:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‚Äò/Users/dikshant/Documents/PlayGround/nanochat-learning/data/shakespeare/input.txt‚Äô\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2026-02-11 18:01:51 (16.8 MB/s) - ‚Äò/Users/dikshant/Documents/PlayGround/nanochat-learning/data/shakespeare/input.txt‚Äô saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -P /Users/dikshant/Documents/PlayGround/nanochat-learning/data/shakespeare/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dikshant/Documents/PlayGround/nanochat-learning/llm-training-journey/experiments/week1-nanogpt\n"
     ]
    }
   ],
   "source": [
    "import os                                                                                                                                               \n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get current directory and go up to nanochat-learning\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent.parent.parent  # Goes up to nanochat-learning\n",
    "\n",
    "# Build path to data\n",
    "data_path = project_root / 'data/shakespeare/input.txt'\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "# Print the length of characters in dataset\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to say it was for his country he did it to\n",
      "please his mother and to be partly proud; which he\n",
      "is, even till the altitude of his virtue.\n",
      "\n",
      "Second Citizen:\n",
      "What he cannot help in his nature, you account a\n",
      "vice in him. You must in no way say he is covetous.\n",
      "\n",
      "First Citizen:\n",
      "If I must not, I need not be barren of accusations;\n",
      "he hath faults, with surplus, to tire in repetition.\n",
      "What shouts are these? The other side o' the city\n",
      "is risen: why stay we prating here? to the Capitol!\n",
      "\n",
      "All:\n",
      "Come, come.\n",
      "\n",
      "First Citizen:\n",
      "Soft! who comes here?\n",
      "\n",
      "Second Citizen:\n",
      "Worthy Menenius Agrippa; one that hath always loved\n",
      "the people.\n",
      "\n",
      "First Citizen:\n",
      "He's one honest enough: would all the rest were so!\n",
      "\n",
      "MENENIUS:\n",
      "What work's, my countrymen, in hand? where go you\n",
      "With bats and clubs? The matter? speak, I pray you.\n",
      "\n",
      "First Citizen:\n",
      "Our business is not unknown to the senate; they have\n",
      "had inkling this fortnight what we intend to do,\n",
      "which now we'll show 'em in deeds. They say poor\n",
      "suitors have strong breaths: they shall know we\n",
      "have strong arms too.\n",
      "\n",
      "MENENIUS:\n",
      "Why, masters, my good friends, mine honest neighbours,\n",
      "Will you undo yourselves?\n",
      "\n",
      "First Citizen:\n",
      "We cannot, sir, we are undone already.\n",
      "\n",
      "MENENIUS:\n",
      "I tell you, friends, most charitable care\n",
      "Have the patricians of you. For your wants,\n",
      "Your suffering in this dearth, you may as well\n",
      "Strike at the heaven with your staves as lift them\n",
      "Against the Roman state, whose course will on\n",
      "The way it takes, cracking ten thousand curbs\n",
      "Of more strong link asunder than can ever\n",
      "Appear in your impediment. For the dearth,\n",
      "The gods, not the patricians, make it, and\n",
      "Your knees to them, not arms, must help. Alack,\n",
      "You are transported by calamity\n",
      "Thither where more attends you, and you slander\n",
      "The helms o' the state, who care for you like fathers,\n",
      "When you curse them as enemies.\n",
      "\n",
      "First Citizen:\n",
      "Care for us! True, indeed! They ne'er cared for us\n",
      "yet: suffer us to famish, and their store-houses\n",
      "crammed with grain; make edicts for usury, to\n",
      "support usurers; repeal daily any wholesome act\n",
      "established against the rich, and provide more\n",
      "piercing statutes daily, to chain up and restrain\n",
      "the poor. If the wars eat us not up, they will; and\n",
      "there's all the love they bear us.\n",
      "\n",
      "MENENIUS:\n",
      "Either you must\n",
      "Confess yourselves wondrous malicious,\n",
      "Or be accused of folly. I shall tell you\n",
      "A pretty tale: it may be you have heard it;\n",
      "But, since it serves my purpose, I will venture\n",
      "To stale 't a little more.\n",
      "\n",
      "First Citizen:\n",
      "Well, I'll hear it, sir: yet you must not think to\n",
      "fob off our disgrace with a tale: but, an 't please\n",
      "you, deliver.\n",
      "\n",
      "MENENIUS:\n",
      "There was a time when all the body's members\n",
      "Rebell'd against the belly, thus accused it:\n",
      "That only like a gulf it did remain\n",
      "I' the midst o' the body, idle and unactive,\n",
      "Still cupboarding the viand, never bearing\n",
      "Like labour with the rest, where the other instruments\n",
      "Did see and hear, devise, instruct, walk, feel,\n",
      "And, mutually participate, did minister\n",
      "Unto the appetite and affection common\n",
      "Of the whole body. The belly answer'd--\n",
      "\n",
      "First Citizen:\n",
      "Well, sir, what answer made the belly?\n",
      "\n",
      "MENENIUS:\n",
      "Sir, I shall tell you. With a kind of smile,\n",
      "Which ne'er came from the lungs, but even thus--\n",
      "For, look you, I may make the belly smile\n",
      "As well as speak--it tauntingly replied\n",
      "To the discontented members, the mutinous parts\n",
      "That envied his receipt; even so most fitly\n",
      "As you malign our senators for that\n",
      "They are not such as you.\n",
      "\n",
      "First Citizen:\n",
      "Your belly's answer? What!\n",
      "The kingly-crowned head, the vigilant eye,\n",
      "The counsellor heart, the arm our soldier,\n",
      "Our steed the leg, the tongue our trumpeter.\n",
      "With other muniments and petty helps\n",
      "In this our fabric, if that they--\n",
      "\n",
      "MENENIUS:\n",
      "What then?\n",
      "'Fore me, this fellow speaks! What then? what then?\n",
      "\n",
      "First Citizen:\n",
      "Should by the cormorant belly be restrain'd,\n",
      "Who is the sink o' the body,--\n",
      "\n",
      "MENENIUS:\n",
      "Well, what then?\n",
      "\n",
      "First Citizen:\n",
      "The former agents, if they did complain,\n",
      "What could the belly answer?\n",
      "\n",
      "MENENIUS:\n",
      "I will tell you\n",
      "If you'll bestow a small--of what you have little--\n",
      "Patience awhile, you'll hear the belly's answer.\n",
      "\n",
      "First Citizen:\n",
      "Ye're long about it.\n",
      "\n",
      "MENENIUS:\n",
      "Note me this, good friend;\n",
      "Your most grave belly was deliberate,\n",
      "Not rash like his accusers, and thus answer'd:\n",
      "'True is it, my incorporate friends,' quoth he,\n",
      "'That I receive the general food at first,\n",
      "Which you do live upon; and fit it is,\n",
      "Because I am the store-house and the shop\n",
      "Of the whole body: but, if you do remember,\n",
      "I send it through the rivers of your blood,\n",
      "Even to the court, the heart, to the seat o' the brain;\n",
      "And, through the cranks and offices of man,\n",
      "The strongest nerves and small inferior veins\n",
      "From me receive that natural competency\n",
      "Whereby they live: and though that all at once,\n",
      "You, my good friends,'--this says the belly, mark me,--\n",
      "\n",
      "First Citizen:\n",
      "Ay, sir; well, well.\n",
      "\n",
      "MENENIUS:\n",
      "'Though all at once cannot\n",
      "See what I do deliver out to each,\n",
      "Yet I can make my audit up, that all\n",
      "From me do back receive the flour of all,\n",
      "And leave me but the bran.' What say you to't?\n",
      "\n",
      "First Citizen:\n",
      "It was an answer: how apply you this?\n",
      "\n",
      "MENENIUS:\n",
      "The senators of Rome are this good belly,\n",
      "And you the mutinous members; for examine\n",
      "Their counsels and their cares, digest things rightly\n",
      "Touching the weal o' the common, you shall find\n",
      "No public benefit which you receive\n",
      "But it proceeds or comes from them to you\n",
      "And no way from yourselves. What do you think,\n",
      "You, the great toe of this assembly?\n",
      "\n",
      "First Citizen:\n",
      "I the great toe! why the great toe?\n",
      "\n",
      "MENENIUS:\n",
      "For that, being one o' the lowest, basest, poorest,\n",
      "Of this most wise rebellion, thou go'st foremost:\n",
      "Thou rascal, that art worst in blood to run,\n",
      "Lead'st first to win some vantage.\n",
      "But make you ready your stiff bats and clubs:\n",
      "Rome and her rats are at the point of battle;\n",
      "The one side must have bale.\n",
      "Hail, noble Marcius!\n",
      "\n",
      "MARCIUS:\n",
      "Thanks. What's the matter, you dissentious rogues,\n",
      "That, rubbing the poor itch of your opinion,\n",
      "Make yourselves scabs?\n",
      "\n",
      "First Citizen:\n",
      "We have ever your good word.\n",
      "\n",
      "MARCIUS:\n",
      "He that will give good words to thee will flatter\n",
      "Beneath abhorring. What would you have, you curs,\n",
      "That like nor peace nor war? the one affrights you,\n",
      "The other makes you proud. He that trusts to you,\n",
      "Where he should find you lions, finds you hares;\n",
      "Where foxes, geese: you are no surer, no,\n",
      "Than is the coal of fire upon the ice,\n",
      "Or hailstone in the sun. Your virtue is\n",
      "To make him worthy whose offence subdues him\n",
      "And curse that justice did it.\n",
      "Who deserves greatness\n",
      "Deserves your hate; and your affections are\n",
      "A sick man's appetite, who desires most that\n",
      "Which would increase his evil. He that depends\n",
      "Upon your favours swims with fins of lead\n",
      "And hews down oaks with rushes. Hang ye! Trust Ye?\n",
      "With every minute you do change a mind,\n",
      "And call him noble that was now your hate,\n",
      "Him vile that was your garland. What's the matter,\n",
      "That in these several places of the city\n",
      "You cry against the noble senate, who,\n",
      "Under the gods, keep you in awe, which else\n",
      "Would feed on one another? What's their seeking?\n",
      "\n",
      "MENENIUS:\n",
      "For corn at their own rates; whereof, they say,\n",
      "The city is well stored.\n",
      "\n",
      "MARCIUS:\n",
      "Hang 'em! They say!\n",
      "They'll sit by the fire, and presume to know\n",
      "What's done i' the Capitol; who's like to rise,\n",
      "Who thrives and who declines; side factions\n",
      "and give out\n",
      "Conjectural marriages; making parties strong\n",
      "And feebling such as stand not in their liking\n",
      "Below their cobbled shoes. They say there's\n",
      "grain enough!\n",
      "Would the nobility lay aside their ruth,\n",
      "And let me use my sword, I'll make a quarry\n",
      "With thousands of these quarter'd slaves, as high\n",
      "As I could pick my lance.\n",
      "\n",
      "MENENIUS:\n",
      "Nay, these are almost thoroughly persuaded;\n",
      "For though abundantly they lack discretion,\n",
      "Yet are they passing cowardly. But, I beseech you,\n",
      "What says the other troop?\n",
      "\n",
      "MARCIUS:\n",
      "They are dissolved: hang 'em!\n",
      "They said they were an-hungry; sigh'd forth proverbs,\n",
      "That hunger broke stone walls, that dogs must eat,\n",
      "That meat was made for mouths, that the gods sent not\n",
      "Corn for the rich men only: with these shreds\n",
      "They vented their complainings; which being answer'd,\n",
      "And a petition granted them, a strange one--\n",
      "To break the heart of generosity,\n",
      "And make bold power look pale--they threw their caps\n",
      "As they would hang them on the horns o' the moon,\n",
      "Shouting their emulation.\n",
      "\n",
      "MENENIUS:\n",
      "What is granted them?\n",
      "\n",
      "MARCIUS:\n",
      "Five tribunes to defend their vulgar wisdoms,\n",
      "Of their own choice: one's Junius Brutus,\n",
      "Sicinius Velutus, and I know not--'Sdeath!\n",
      "The rabble should have first unroof'd the city,\n",
      "Ere so prevail'd with me: \n"
     ]
    }
   ],
   "source": [
    "# Look at the first 10000 characters\n",
    "print(text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are all the characters in the vocabulary of the input:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "\n",
      "The number of unique characters we have in our vocabulary:  65\n"
     ]
    }
   ],
   "source": [
    "# Check out all the unique characters in the text\n",
    "\n",
    "# Extract all unique characters from text and sort them alphabetically  \n",
    "chars = sorted(list(set(text))) # print each function output in this line if needed\n",
    "vocab_size = len(chars) # Possible elements in our sequences\n",
    "\n",
    "print(\"The following are all the characters in the vocabulary of the input: \", ''.join(chars))\n",
    "print(\"\\n\")\n",
    "print(\"The number of unique characters we have in our vocabulary: \",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 TOKENIZATION OF TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The char i is encoded as integer:  [47]\n",
      "The char i is decoded back from integer value [47]  to  i\n",
      "\n",
      "\n",
      "\n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "stoi = {ch : i for i,ch in enumerate(chars) }\n",
    "itos = {i : ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s : [stoi[c] for c in s ] # encoder: take a string, output a list of integers\n",
    "decode = lambda l : ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(\"The char i is encoded as integer: \",encode(\"i\"))\n",
    "print(\"The char i is decoded back from integer value\",encode(\"i\"),\" to \",decode(encode(\"i\")))\n",
    "print(\"\\n\\n\")\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is  torch.Size([1115394]) \n",
      "\n",
      "The data type of each values in the data object is  torch.int64 \n",
      "\n",
      "The 1st 1000 character encoding in the data object looks like  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire text input dataset and store it into a torch.Tensor\n",
    "\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long) # Why the long datatype?\n",
    "\n",
    "print(\"The shape of the data is \", data.shape, \"\\n\") #check the data shape\n",
    "print(\"The data type of each values in the data object is \" , data.dtype, \"\\n\" ) # check the datatype\n",
    "\n",
    "print(\"The 1st 1000 character encoding in the data object looks like \", data[:1000]) # check the encoding of the first 1000 characters of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Split the Dataset into Training Dataset and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation Dataset\n",
    "\n",
    "n = int(0.9 * len(data)) # First 90% train dataset, the rest will be validation dataset\n",
    "\n",
    "train_data = data[:n] # Creating Training Dataset # Dataset the model is trained on\n",
    "val_data = data[n:] # Creating Validation Dataset # Dataset that helps us test how much we are overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Sample Train Data\n",
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Sample Val Data\n",
    "val_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE for Next Steps:\n",
    "- Now we will start to train the Transformer.\n",
    "- We won't feed the entire text (i.e train_data) to the transformer. Why?\n",
    "- We feed only chunks of data to the transformer. These chunks of data are randomly picked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Idea about block Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # This is also known as Context Window\n",
    "\n",
    "train_data[: block_size + 1] # View the dataset in block sizes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NOTE #1: The above block has multiple examples packed into it that is because all the characters Follow each other\n",
    "######        We are simultaneously training the model to make predictions in each of these positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# Divide the training data into x which are our features and y that is our actual output that we want to predict\n",
    "x = train_data[ : block_size]\n",
    "y = train_data[1: block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[ : t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NOTE #1 Continued: **As we can see from the above output, we are getting 8 separate training samples hiding into one chunk**\n",
    "######      \n",
    "###### ** We train all the elements in one block together not jsut for efficiency but also because we want the Transformer used to seeing the context from one to eight. Meaning it is used to see one character and then make prediction, two chars and then make the prediction, and so on **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We looked at the time dimension of the Tensors that we are feeding into the Transformer. Now, we will look\n",
    "### at the Batch Dimension\n",
    "\n",
    "### IDEA of Batch Dimension: \n",
    "- We will have multiple batches of chunks of text that we are going to feed into the transformer.\n",
    "- These batches will be stacked up in a single tensor. That's done for efficiency. GPUs are very good at parallel processing of data.\n",
    "- These chunks are processed independently and they don't talk to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Introducing Batch Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS: \n",
      "The shape of xb is:  torch.Size([4, 8])\n",
      "The values of xb are:  tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "TARGETS: \n",
      "The shape of yb is:  torch.Size([4, 8])\n",
      "The values of yb are:  tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "------\n",
      "Batch 0 | Time 0 | when input is [24] the target is: 43\n",
      "Batch 0 | Time 1 | when input is [24, 43] the target is: 58\n",
      "Batch 0 | Time 2 | when input is [24, 43, 58] the target is: 5\n",
      "Batch 0 | Time 3 | when input is [24, 43, 58, 5] the target is: 57\n",
      "Batch 0 | Time 4 | when input is [24, 43, 58, 5, 57] the target is: 1\n",
      "Batch 0 | Time 5 | when input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "Batch 0 | Time 6 | when input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "Batch 0 | Time 7 | when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "Batch 1 | Time 0 | when input is [44] the target is: 53\n",
      "Batch 1 | Time 1 | when input is [44, 53] the target is: 56\n",
      "Batch 1 | Time 2 | when input is [44, 53, 56] the target is: 1\n",
      "Batch 1 | Time 3 | when input is [44, 53, 56, 1] the target is: 58\n",
      "Batch 1 | Time 4 | when input is [44, 53, 56, 1, 58] the target is: 46\n",
      "Batch 1 | Time 5 | when input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "Batch 1 | Time 6 | when input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "Batch 1 | Time 7 | when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "Batch 2 | Time 0 | when input is [52] the target is: 58\n",
      "Batch 2 | Time 1 | when input is [52, 58] the target is: 1\n",
      "Batch 2 | Time 2 | when input is [52, 58, 1] the target is: 58\n",
      "Batch 2 | Time 3 | when input is [52, 58, 1, 58] the target is: 46\n",
      "Batch 2 | Time 4 | when input is [52, 58, 1, 58, 46] the target is: 39\n",
      "Batch 2 | Time 5 | when input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "Batch 2 | Time 6 | when input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "Batch 2 | Time 7 | when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "Batch 3 | Time 0 | when input is [25] the target is: 17\n",
      "Batch 3 | Time 1 | when input is [25, 17] the target is: 27\n",
      "Batch 3 | Time 2 | when input is [25, 17, 27] the target is: 10\n",
      "Batch 3 | Time 3 | when input is [25, 17, 27, 10] the target is: 0\n",
      "Batch 3 | Time 4 | when input is [25, 17, 27, 10, 0] the target is: 21\n",
      "Batch 3 | Time 5 | when input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "Batch 3 | Time 6 | when input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "Batch 3 | Time 7 | when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# SEED & HYPERPARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "# Think of this like telling a dice:\n",
    "# \"Every time I roll you, give me the same sequence of numbers\"\n",
    "# This means everyone running this code sees the exact same results\n",
    "# Without this, every run gives different random numbers ‚Üí hard to debug\n",
    "\n",
    "batch_size = 4\n",
    "# How many independent sequences we process at the same time\n",
    "# Think of it as: 4 students each reading a different page of a book\n",
    "# They all do their work simultaneously ‚Üí faster training\n",
    "# More batch_size = faster BUT needs more GPU memory\n",
    "\n",
    "block_size = 8\n",
    "# The maximum context window ‚Äî how far back the model can see\n",
    "# To predict the next token, the model can look at upto 8 previous tokens\n",
    "# Example: to predict token 9, it can look at tokens 1,2,3,4,5,6,7,8\n",
    "\n",
    "# ============================================================\n",
    "# GET BATCH FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Description: Grabs a random batch of chunks from the dataset.\n",
    "                 Returns inputs (x) and their corresponding targets (y)\n",
    "    \n",
    "    Input:  split ‚Üí either the string 'train' or 'val'\n",
    "                    tells us which dataset to pull from\n",
    "    \n",
    "    Output: x ‚Üí input sequences  of shape [batch_size √ó block_size] = [4 √ó 8]\n",
    "            y ‚Üí target sequences of shape [batch_size √ó block_size] = [4 √ó 8]\n",
    "                y is just x shifted by 1 position to the right\n",
    "                because y[t] is always the answer to \"what comes after x[t]?\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Pick the correct dataset based on the split argument\n",
    "    # If we are training ‚Üí use train_data\n",
    "    # If we are evaluating ‚Üí use val_data\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # ‚îÄ‚îÄ UNDERSTANDING ix ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # Imagine your data is a long strip of 1,000,000 tokens:\n",
    "    # [ h e l l o ' ' C l i t i z e n ... K a n e ... ]\n",
    "    #   0 1 2 3 4  5  6 7 8 9 10 11 12     ‚Üë           \n",
    "    #                                  some random position\n",
    "    #\n",
    "    # We throw 4 random darts at this strip to get 4 starting positions\n",
    "    # BUT we must not start too close to the end ‚Äî otherwise we fall off!\n",
    "    # \n",
    "    # WRONG ‚ùå ‚Äî starting at position 999,998 and asking for 8 tokens\n",
    "    # RIGHT ‚úÖ ‚Äî last safe start = len(data) - block_size = 999,992\n",
    "    #\n",
    "    # torch.randint(N, (batch_size,)) means:\n",
    "    # \"give me (batch_size=4) random integers between 0 and N\"\n",
    "    # Result looks like: ix = [892, 4521, 7634, 1023]\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # ‚îÄ‚îÄ UNDERSTANDING x ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # For each starting position i in ix, grab the next 8 tokens\n",
    "    # \n",
    "    # i=892  ‚Üí data[892  : 900 ] ‚Üí [y, o, u, ' ', a, n, d, ' ']\n",
    "    # i=4521 ‚Üí data[4521 : 4529] ‚Üí [F, i, r, s,  t, ' ', C, i]\n",
    "    # i=7634 ‚Üí data[7634 : 7642] ‚Üí [L, E, O, N,  T, E,  S, ' ']\n",
    "    # i=1023 ‚Üí data[1023 : 1031] ‚Üí [K, a, n, e, ' ', t, h, e ]\n",
    "    #\n",
    "    # torch.stack() piles these 4 strips on top of each other like pancakesü•û\n",
    "    # turning a list of 4 tensors (each of size 8) into ONE matrix of [4 √ó 8]\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "\n",
    "    # ‚îÄ‚îÄ UNDERSTANDING y ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    # EXACTLY the same as x but shifted 1 position to the right\n",
    "    # Because y is our answer key ‚Äî \"what token should come AFTER each x token?\"\n",
    "    #\n",
    "    # i=892  ‚Üí data[893  : 901 ] ‚Üí [o, u, ' ', a, n, d, ' ', I]\n",
    "    # i=4521 ‚Üí data[4522 : 4530] ‚Üí [i, r, s,  t, ' ', C, i, t]\n",
    "    # i=7634 ‚Üí data[7635 : 7643] ‚Üí [E, O, N,  T, E,  S, ' ', L]\n",
    "    # i=1023 ‚Üí data[1024 : 1032] ‚Üí [a, n, e, ' ', t, h, e, ' ']\n",
    "    #\n",
    "    # x asks the question ‚îÄ‚îÄ‚Üí y holds the answer\n",
    "    # They are the same data, just offset by 1 position\n",
    "    # That single +1 shift is the entire secret of language model training!\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# ============================================================\n",
    "# CALL THE FUNCTION & INSPECT THE OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "# xb = our inputs  ‚Üí shape: [4, 8] ‚Üí 4 sequences, each 8 tokens long\n",
    "# yb = our targets ‚Üí shape: [4, 8] ‚Üí 4 answer keys, each 8 tokens long\n",
    "\n",
    "print(\"INPUTS: \")\n",
    "print(\"The shape of xb is: \", xb.shape)\n",
    "# Prints ‚Üí torch.Size([4, 8])\n",
    "# Read as: 4 sequences (batch dimension) √ó 8 tokens (time dimension)\n",
    "\n",
    "print(\"The values of xb are: \", xb)\n",
    "# Prints the actual token IDs (integers) inside the matrix\n",
    "\n",
    "print(\"TARGETS: \")\n",
    "print(\"The shape of yb is: \", yb.shape)\n",
    "# Also ‚Üí torch.Size([4, 8]) ‚Äî same shape as xb, just shifted by 1\n",
    "\n",
    "print(\"The values of yb are: \", yb)\n",
    "# Prints the target token IDs\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "# ============================================================\n",
    "# UNPACKING ALL TRAINING EXAMPLES INSIDE THE BATCH\n",
    "# ============================================================\n",
    "# Remember: inside each [4 √ó 8] matrix, there are actually\n",
    "# 4 √ó 8 = 32 individual training examples hidden inside!\n",
    "# This loop unpacks and prints every single one of them\n",
    "\n",
    "for b in range(batch_size):     # Loop over BATCH dimension ‚Üí which sequence? (0,1,2,3)\n",
    "    for t in range(block_size): # Loop over TIME dimension  ‚Üí where in the sequence? (0‚Üí7)\n",
    "\n",
    "        # xb[b, :t+1] means:\n",
    "        # ‚Üí go to row b       (pick the b-th sequence out of our 4)\n",
    "        # ‚Üí grab columns 0‚Üít  (grab tokens from start up to position t)\n",
    "        # As t grows from 0 to 7, the context gets longer and longer\n",
    "        # t=0 ‚Üí context is just 1 token\n",
    "        # t=7 ‚Üí context is all 8 tokens\n",
    "        context = xb[b, :t+1]\n",
    "\n",
    "        # yb[b, t] means:\n",
    "        # ‚Üí go to row b       (same sequence)\n",
    "        # ‚Üí grab column t     (the single target token at position t)\n",
    "        # This is what the model should predict given the context above\n",
    "        target = yb[b, t]\n",
    "\n",
    "        print(f\"Batch {b} | Time {t} | when input is {context.tolist()} the target is: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These are the 32 input examples packed into one batch of tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of what the Output Looks Like for Batch 0 as Characters\n",
    "\n",
    "| Batch | Time | when input is | target |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 0 | `[y]` | `o` |\n",
    "| 0 | 1 | `[y, o]` | `u` |\n",
    "| 0 | 2 | `[y, o, u]` | `' '` |\n",
    "| 0 | 3 | `[y, o, u, ' ']` | `a` |\n",
    "| 0 | 4 | `[y, o, u, ' ', a]` | `n` |\n",
    "| 0 | 5 | `[y, o, u, ' ', a, n]` | `d` |\n",
    "| 0 | 6 | `[y, o, u, ' ', a, n, d]` | `' '` |\n",
    "| 0 | 7 | `[y, o, u, ' ', a, n, d, ' ']` | `I` |\n",
    "\n",
    "#### Then the same pattern repeats for Batch 1, 2, and 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NEXT STEP: Now that we have our batch of inputs and outputs, let's feed them in to the transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bigram Model\n",
    "\n",
    "Simple baseline model that predicts next character based only on current character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement bigram language model\n",
    "# Train and generate samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the logits is torch.Size([32, 65])\n",
      "The loss is: tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "# üé≤ Fixes the random number generator so results are the same every run.\n",
    "# Like always rolling the same dice sequence. Good for debugging.\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    # nn.Module is PyTorch's base blueprint for ALL neural networks.\n",
    "    # By writing (nn.Module) we inherit all of PyTorch's standard machinery.\n",
    "    # Think of it as: \"I want to build MY model on top of PyTorch's foundation.\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        DESCRIPTION:\n",
    "            The setup function. Runs ONCE when you create the model.\n",
    "            Builds the lookup table (the only thing this model learns).\n",
    "            Think of it as: \"Build the kitchen before you start cooking.\"\n",
    "\n",
    "        INPUT:\n",
    "            vocab_size ‚Üí how many unique tokens exist in our language.\n",
    "                         In our small example: 4 (tokens: 'a','b','c','d')\n",
    "                         In Karpathy's Shakespeare model: 65 characters.\n",
    "\n",
    "        OUTPUT:\n",
    "            None. Just sets up the model's internal structure.\n",
    "            After this runs, the model exists but knows nothing yet.\n",
    "            The lookup table starts with random garbage numbers.\n",
    "        \"\"\"\n",
    "        # Runs ONCE when you create the model. Sets up the \"kitchen.\"\n",
    "        #\n",
    "        # In our small example:\n",
    "        #   vocab_size = 4  (only 4 tokens exist: 'a'=0, 'b'=1, 'c'=2, 'd'=3)\n",
    "\n",
    "        super().__init__()\n",
    "        # Tells PyTorch's nn.Module to do ITS setup first.\n",
    "        # It prepares internal bookkeeping:\n",
    "        #   ‚Üí tracks all learnable parameters\n",
    "        #   ‚Üí enables .to(device), .parameters(), .train(), .eval()\n",
    "        # Skip this line and PyTorch throws an error immediately.\n",
    "        # Rule: ALWAYS call this first in __init__. No exceptions.\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # Creates a LOOKUP TABLE of shape [vocab_size √ó vocab_size]\n",
    "        # In our example: a 4 √ó 4 table\n",
    "        #\n",
    "        #   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        #   ‚îÇ           next token scores (C=4 columns)       ‚îÇ\n",
    "        #   ‚îÇ         'a'    'b'    'c'    'd'                 ‚îÇ\n",
    "        #   ‚îÇ  'a'(0) [0.1,  0.8,  0.3,  0.5]  ‚Üê row 0       ‚îÇ\n",
    "        #   ‚îÇ  'b'(1) [0.6,  0.2,  0.9,  0.1]  ‚Üê row 1       ‚îÇ\n",
    "        #   ‚îÇ  'c'(2) [0.4,  0.7,  0.2,  0.6]  ‚Üê row 2       ‚îÇ\n",
    "        #   ‚îÇ  'd'(3) [0.9,  0.1,  0.5,  0.3]  ‚Üê row 3       ‚îÇ\n",
    "        #   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        #\n",
    "        # Each ROW = one token's opinion of what comes next.\n",
    "        # e.g. Row for 'b' = [0.6, 0.2, 0.9, 0.1]\n",
    "        #      ‚Üí model currently thinks 'c' (score=0.9) most likely follows 'b'\n",
    "        #\n",
    "        # These numbers START as random garbage (normal distribution, mean=0, std=1).\n",
    "        # During training, backprop nudges these numbers to be less wrong.\n",
    "        # THIS TABLE is the ONLY thing the Bigram model learns. That's it.\n",
    "        #\n",
    "        # Note: vocab_size ‚â† T (block_size/context window)!\n",
    "        # vocab_size (C=4) = how many unique tokens EXIST in the language\n",
    "        # T          (=3)  = how long each sequence is\n",
    "        # These are completely independent. Like:\n",
    "        #   English has 26 letters (vocab_size=26)\n",
    "        #   \"cat\" is 3 letters long (T=3)\n",
    "        #   T does NOT have to equal 26!\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        DESCRIPTION:\n",
    "            The prediction function. Runs every time you call m(xb, yb).\n",
    "            Takes a sequence of token IDs, looks each one up in the table,\n",
    "            and returns a score for every possible next token.\n",
    "            Optionally computes loss if targets are provided.\n",
    "\n",
    "            Two modes:\n",
    "              TRAINING mode   ‚Üí call m(xb, yb)  ‚Üí returns logits + loss\n",
    "              GENERATION mode ‚Üí call self(idx)   ‚Üí returns logits + None\n",
    "\n",
    "        INPUT:\n",
    "            idx     ‚Üí (B, T) tensor of token IDs ‚Äî the input sequences.\n",
    "                      B = batch_size = how many sequences at once.\n",
    "                      T = block_size = how long each sequence is.\n",
    "                      In our example: B=2, T=3.\n",
    "\n",
    "                      Example:\n",
    "                                t=0  t=1  t=2\n",
    "                      seq 0:  [  0,   1,   2 ]  ‚Üí tokens: a, b, c\n",
    "                      seq 1:  [  3,   0,   1 ]  ‚Üí tokens: d, a, b\n",
    "\n",
    "            targets ‚Üí (B, T) tensor of correct next token IDs. OPTIONAL.\n",
    "                      Same shape as idx but shifted one step forward.\n",
    "                      \"What token SHOULD come after each position?\"\n",
    "\n",
    "                      Example:\n",
    "                                t=0  t=1  t=2\n",
    "                      seq 0:  [  1,   2,   3 ]  ‚Üí expected next: b, c, d\n",
    "                      seq 1:  [  0,   1,   2 ]  ‚Üí expected next: a, b, c\n",
    "\n",
    "        OUTPUT:\n",
    "            logits ‚Üí (B*T, C) tensor of raw prediction scores.\n",
    "                     One row per token position, C scores per row.\n",
    "                     In our example: shape (6, 4).\n",
    "\n",
    "            loss   ‚Üí single number measuring how wrong the predictions are.\n",
    "                     Lower is better. Starts near log(vocab_size) for a\n",
    "                     random model. e.g. log(4) ‚âà 1.38 in our example.\n",
    "                     Returns None if no targets were provided.\n",
    "        \"\"\"\n",
    "        # Called every time you run the model on data.\n",
    "        # Defines: \"given input tokens, how do we compute predictions?\"\n",
    "        # PyTorch calls this automatically when you do m(xb, yb).\n",
    "        #\n",
    "        # targets=None makes targets OPTIONAL:\n",
    "        #   During TRAINING:    m(xb, yb)  ‚Üí targets given   ‚Üí loss calculated\n",
    "        #   During GENERATION:  self(idx)  ‚Üí no targets       ‚Üí loss skipped\n",
    "        #\n",
    "        # ‚îÄ‚îÄ INPUTS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # idx     shape: (B, T) ‚Üí input token IDs\n",
    "        # targets shape: (B, T) ‚Üí correct next tokens (what we want to predict)\n",
    "        #\n",
    "        # B = batch_size = number of sequences processed in parallel\n",
    "        # T = block_size = context window = how long each sequence is\n",
    "        #\n",
    "        # In our example: B=2, T=3\n",
    "        #\n",
    "        # idx looks like this ‚Äî a (2 √ó 3) grid of token IDs:\n",
    "        #\n",
    "        #             t=0   t=1   t=2\n",
    "        # sequence 0: [ 0,    1,    2 ]  ‚Üê tokens: a, b, c\n",
    "        # sequence 1: [ 3,    0,    1 ]  ‚Üê tokens: d, a, b\n",
    "        #\n",
    "        # targets is the SAME shape but shifted one step forward:\n",
    "        # \"what token should come AFTER each position?\"\n",
    "        #\n",
    "        #             t=0   t=1   t=2\n",
    "        # sequence 0: [ 1,    2,    3 ]  ‚Üê expected next: b, c, d\n",
    "        # sequence 1: [ 0,    1,    2 ]  ‚Üê expected next: a, b, c\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        # For EVERY token ID in idx, go to the table and grab its ROW.\n",
    "        # Each row has C=vocab_size numbers (scores for each possible next token).\n",
    "        #\n",
    "        # idx was (B=2, T=3) ‚Äî a flat grid of integers\n",
    "        # logits becomes (B=2, T=3, C=4) ‚Äî a 3D cube of scores\n",
    "        #\n",
    "        # What happened? Each integer got SWAPPED with its full row of C scores:\n",
    "        #\n",
    "        #   idx[0][0] = 0 (token 'a') ‚Üí look up row 0 ‚Üí [0.1, 0.8, 0.3, 0.5]\n",
    "        #   idx[0][1] = 1 (token 'b') ‚Üí look up row 1 ‚Üí [0.6, 0.2, 0.9, 0.1]\n",
    "        #   idx[0][2] = 2 (token 'c') ‚Üí look up row 2 ‚Üí [0.4, 0.7, 0.2, 0.6]\n",
    "        #   idx[1][0] = 3 (token 'd') ‚Üí look up row 3 ‚Üí [0.9, 0.1, 0.5, 0.3]\n",
    "        #   idx[1][1] = 0 (token 'a') ‚Üí look up row 0 ‚Üí [0.1, 0.8, 0.3, 0.5]\n",
    "        #   idx[1][2] = 1 (token 'b') ‚Üí look up row 1 ‚Üí [0.6, 0.2, 0.9, 0.1]\n",
    "        #\n",
    "        # These scores are called LOGITS ‚Äî raw, unnormalized predictions.\n",
    "        # They are NOT yet probabilities. To convert: apply softmax.\n",
    "        # The highest logit = the model's current best guess for next token.\n",
    "        #\n",
    "        # Example: logits[0][1] = [0.6, 0.2, 0.9, 0.1]\n",
    "        #                              a    b    c    d\n",
    "        #                                        ‚Üë\n",
    "        #                          Highest! Model guesses 'c' follows 'b'\n",
    "\n",
    "        # ‚îÄ‚îÄ RESHAPE FOR LOSS CALCULATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "        if targets is None:\n",
    "            # We land here during GENERATION (no targets provided).\n",
    "            # No targets = nothing to compare against = no loss to calculate.\n",
    "            # Just return the raw logits and None for loss.\n",
    "            loss = None\n",
    "        else:\n",
    "            # We land here during TRAINING (targets provided).\n",
    "            # Now we can measure how wrong our predictions are.\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            # Unpack the 3 dimensions of our cube into separate variables.\n",
    "            # logits.shape = (2, 3, 4) ‚Üí B=2, T=3, C=4\n",
    "            # We need these as separate numbers for the reshape step below.\n",
    "\n",
    "            logits = logits.view(B*T, C)\n",
    "            # RESHAPE logits from 3D cube ‚Üí 2D flat table.\n",
    "            # (B=2, T=3, C=4) ‚Üí (B*T=6, C=4)\n",
    "            #\n",
    "            # WHY? F.cross_entropy strictly requires C in the SECOND position.\n",
    "            # It just wants a simple list of predictions ‚Äî it doesn't care\n",
    "            # about batches or sequences. So we flatten B and T into one.\n",
    "            # B*T = 2*3 = 6 total individual predictions.\n",
    "            #\n",
    "            #   BEFORE ‚Äî 3D cube (B=2 pages, T=3 rows, C=4 scores):\n",
    "            #\n",
    "            #   Page B=0 (seq: a,b,c):        Page B=1 (seq: d,a,b):\n",
    "            #   t=0: [.1, .8, .3, .5] ‚Üê a     t=0: [.9, .1, .5, .3] ‚Üê d\n",
    "            #   t=1: [.6, .2, .9, .1] ‚Üê b     t=1: [.1, .8, .3, .5] ‚Üê a\n",
    "            #   t=2: [.4, .7, .2, .6] ‚Üê c     t=2: [.6, .2, .9, .1] ‚Üê b\n",
    "            #\n",
    "            #   AFTER view(B*T, C) ‚Äî 2D table (6 rows, 4 scores each):\n",
    "            #\n",
    "            #   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            #   ‚îÇ      c=0   c=1   c=2   c=3              ‚îÇ\n",
    "            #   ‚îÇ  0: [ .1,   .8,   .3,   .5 ] ‚Üê B=0,t=0 ‚îÇ  (was token 'a')\n",
    "            #   ‚îÇ  1: [ .6,   .2,   .9,   .1 ] ‚Üê B=0,t=1 ‚îÇ  (was token 'b')\n",
    "            #   ‚îÇ  2: [ .4,   .7,   .2,   .6 ] ‚Üê B=0,t=2 ‚îÇ  (was token 'c')\n",
    "            #   ‚îÇ  3: [ .9,   .1,   .5,   .3 ] ‚Üê B=1,t=0 ‚îÇ  (was token 'd')\n",
    "            #   ‚îÇ  4: [ .1,   .8,   .3,   .5 ] ‚Üê B=1,t=1 ‚îÇ  (was token 'a')\n",
    "            #   ‚îÇ  5: [ .6,   .2,   .9,   .1 ] ‚Üê B=1,t=2 ‚îÇ  (was token 'b')\n",
    "            #   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            #\n",
    "            # ZERO data changed. Just reorganized. Like unfolding a box flat.\n",
    "\n",
    "            targets = targets.view(B*T)\n",
    "            # RESHAPE targets from 2D grid ‚Üí 1D flat list.\n",
    "            # (B=2, T=3) ‚Üí (B*T=6,)\n",
    "            #\n",
    "            # Must match logits exactly ‚Äî 6 predictions need 6 correct answers.\n",
    "            #\n",
    "            #   BEFORE ‚Äî 2D grid (2 √ó 3):     AFTER view(B*T) ‚Äî 1D list (6,):\n",
    "            #\n",
    "            #             t=0  t=1  t=2\n",
    "            #   seq 0:  [  1,   2,   3  ]  ‚Üí  [ 1, 2, 3, 0, 1, 2 ]\n",
    "            #   seq 1:  [  0,   1,   2  ]       ‚Üë  ‚Üë  ‚Üë  ‚Üë  ‚Üë  ‚Üë\n",
    "            #                                  B0 B0 B0 B1 B1 B1\n",
    "            #                                  t0 t1 t2 t0 t1 t2\n",
    "            #\n",
    "            # targets.view(-1) does the EXACT same thing.\n",
    "            # The -1 means \"figure out the size yourself.\"\n",
    "            # PyTorch sees 6 total elements ‚Üí fills in -1 as 6.\n",
    "            # Both are correct. view(-1) is just shorter to write.\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # Measures HOW WRONG our predictions are. Returns ONE number.\n",
    "            #\n",
    "            # For each of the 6 predictions, it asks:\n",
    "            # \"Is the CORRECT next token's score the HIGHEST score?\"\n",
    "            #\n",
    "            #   Prediction 0 ‚Äî token 'a', target='b'(id=1):\n",
    "            #   logits: [ .1,  .8,  .3,  .5 ]\n",
    "            #              a    b    c    d\n",
    "            #                   ‚Üë target 'b' has score .8 ‚Üí highest ‚úÖ ‚Üí LOW loss\n",
    "            #\n",
    "            #   Prediction 2 ‚Äî token 'c', target='d'(id=3):\n",
    "            #   logits: [ .4,  .7,  .2,  .6 ]\n",
    "            #              a    b    c    d\n",
    "            #                             ‚Üë target 'd' score=.6 ‚Üí NOT highest ‚ùå\n",
    "            #                             ‚Üí 'b' (.7) is higher ‚Üí HIGH loss\n",
    "            #\n",
    "            # The TOTAL loss = average across all 6 predictions.\n",
    "            #\n",
    "            # Internally cross_entropy does 3 steps:\n",
    "            #   Step 1 ‚Äî softmax:  raw scores ‚Üí probabilities (sum to 100%)\n",
    "            #   Step 2 ‚Äî pick:     grab ONLY the probability of the correct token\n",
    "            #   Step 3 ‚Äî -log():   convert that probability to a loss number\n",
    "            #\n",
    "            #   Why -log()? Because it has the perfect shape:\n",
    "            #\n",
    "            #   Prob of correct token    -log(p)    Meaning\n",
    "            #   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            #         100%          ‚Üí      0.0      üéâ Perfect\n",
    "            #          50%          ‚Üí      0.69     üòê Okay\n",
    "            #          25%          ‚Üí      1.38     üòü Bad ‚Üê random model starts HERE\n",
    "            #          10%          ‚Üí      2.30     üò± Very bad\n",
    "            #           1%          ‚Üí      4.60     üíÄ Terrible\n",
    "            #\n",
    "            # Untrained random model ‚Üí all 4 tokens get ~25% probability each\n",
    "            # ‚Üí expected starting loss = -log(0.25) = log(4) ‚âà 1.38\n",
    "            # This is your SANITY CHECK. Always verify this before training!\n",
    "\n",
    "        return logits, loss\n",
    "        # Returns TWO things:\n",
    "        #   logits ‚Üí raw predictions\n",
    "        #   loss   ‚Üí how wrong we are (None if no targets were given)\n",
    "        #\n",
    "        # Training loop will use loss to do backprop ‚Üí update the table\n",
    "        # ‚Üí next time loss will be slightly lower ‚Üí repeat thousands of times.\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        DESCRIPTION:\n",
    "            The text generation function.\n",
    "            Takes a starting seed token and grows the sequence\n",
    "            one token at a time, max_new_tokens times.\n",
    "            Like giving the model a single letter and asking it\n",
    "            to keep writing from there.\n",
    "\n",
    "            Each step:\n",
    "              1. Run the current sequence through forward()\n",
    "              2. Look at ONLY the last token's scores\n",
    "              3. Convert scores ‚Üí probabilities via softmax\n",
    "              4. Randomly sample one token from those probabilities üé≤\n",
    "              5. Append that new token to the sequence\n",
    "              6. Repeat\n",
    "\n",
    "        INPUT:\n",
    "            idx            ‚Üí (B, T) tensor of starting token IDs.\n",
    "                             Usually (1, 1) ‚Äî one sequence, one seed token.\n",
    "\n",
    "                             Example: torch.zeros((1,1), dtype=torch.long)\n",
    "                             ‚îå‚îÄ‚îÄ‚îÄ‚îê\n",
    "                             ‚îÇ 0 ‚îÇ  ‚Üê token id=0 = 'a', used as start signal\n",
    "                             ‚îî‚îÄ‚îÄ‚îÄ‚îò\n",
    "                             shape: (B=1, T=1)\n",
    "                             B=1 = 1 sequence in the batch (COUNT, not index!)\n",
    "                             T=1 = that sequence is 1 token long\n",
    "\n",
    "            max_new_tokens ‚Üí how many NEW tokens to generate and add.\n",
    "                             e.g. 100 ‚Üí sequence grows from T=1 to T=101.\n",
    "\n",
    "        OUTPUT:\n",
    "            idx ‚Üí (B, T + max_new_tokens) tensor.\n",
    "                  The original seed tokens PLUS all newly generated tokens.\n",
    "\n",
    "                  Example with max_new_tokens=3, seed='a':\n",
    "                  Start:       [[0]]           shape:(1,1) ‚Üí 'a'\n",
    "                  After loop1: [[0, 3]]        shape:(1,2) ‚Üí 'a','d'\n",
    "                  After loop2: [[0, 3, 1]]     shape:(1,3) ‚Üí 'a','d','b'\n",
    "                  After loop3: [[0, 3, 1, 2]]  shape:(1,4) ‚Üí 'a','d','b','c'\n",
    "        \"\"\"\n",
    "\n",
    "        # In our example: idx starts as (B=1, T=1) ‚Äî one sequence, one token\n",
    "        # e.g. idx = [[0]]  ‚Üí just the token 'a' as a starting seed\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # We repeat this loop max_new_tokens times.\n",
    "            # Each loop iteration = generate ONE new token and add it to idx.\n",
    "            # Think of it like adding one word at a time to a growing sentence.\n",
    "\n",
    "            # ‚îÄ‚îÄ STEP 1: GET PREDICTIONS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            logits, loss = self(idx)\n",
    "            # Run the model on the ENTIRE current sequence.\n",
    "            # No targets needed here ‚Üí loss will be None (that's fine, we ignore it).\n",
    "            # logits shape: (B, T, C) ‚Äî a score vector for every position.\n",
    "            #\n",
    "            # Example after 1st iteration with idx=[[0]] (just token 'a'):\n",
    "            # logits shape: (1, 1, 4)\n",
    "            #   ‚Üí 1 sequence, 1 position, 4 scores\n",
    "\n",
    "            # ‚îÄ‚îÄ STEP 2: FOCUS ONLY ON THE LAST TOKEN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            logits = logits[:, -1, :]\n",
    "            # WHY only the last token?\n",
    "            # Because in a Bigram model, ONLY the most recent token matters.\n",
    "            # It doesn't use older context ‚Äî it purely asks:\n",
    "            # \"given the LAST token I saw, what comes next?\"\n",
    "            #\n",
    "            # logits was (B, T, C) ‚Üí logits[:, -1, :] grabs the LAST time step\n",
    "            # ‚Üí becomes (B, C)\n",
    "            #\n",
    "            # Example: idx = [[0, 1, 2]]  (sequence: a, b, c)\n",
    "            #\n",
    "            #   logits BEFORE [:, -1, :] ‚Äî shape (1, 3, 4):\n",
    "            #   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            #   ‚îÇ t=0 (after 'a'): [.1, .8, .3, .5]  ‚îÇ\n",
    "            #   ‚îÇ t=1 (after 'b'): [.6, .2, .9, .1]  ‚îÇ\n",
    "            #   ‚îÇ t=2 (after 'c'): [.4, .7, .2, .6]  ‚îÇ ‚Üê -1 grabs THIS row\n",
    "            #   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            #\n",
    "            #   logits AFTER [:, -1, :] ‚Äî shape (1, 4):\n",
    "            #   [ .4, .7, .2, .6 ]  ‚Üê just the scores for \"what follows 'c'?\"\n",
    "            #      a    b    c    d\n",
    "            #           ‚Üë highest score ‚Üí model thinks 'b' follows 'c'\n",
    "\n",
    "            # ‚îÄ‚îÄ STEP 3: CONVERT SCORES ‚Üí PROBABILITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Softmax squashes raw logit scores into proper probabilities.\n",
    "            # All values become positive and sum to exactly 1.0 (100%).\n",
    "            # dim=-1 means \"apply softmax along the LAST dimension\" (across C scores).\n",
    "            #\n",
    "            # Example:\n",
    "            #   logits: [ .4,  .7,  .2,  .6 ]   ‚Üê raw scores (don't sum to 1)\n",
    "            #              a    b    c    d\n",
    "            #              ‚Üì softmax\n",
    "            #   probs:  [.22, .30, .18, .27]     ‚Üê now sum to ~1.0 (100%) ‚úÖ\n",
    "            #              a    b    c    d\n",
    "            #            22%  30%  18%  27%\n",
    "            #\n",
    "            # 'b' still has the highest probability (30%) ‚Äî same winner, but\n",
    "            # now expressed as a proper probability we can SAMPLE from.\n",
    "\n",
    "            # ‚îÄ‚îÄ STEP 4: SAMPLE ONE TOKEN FROM THE PROBABILITIES ‚îÄ‚îÄ‚îÄ\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # üé≤ THIS is the ONLY step that introduces randomness.\n",
    "            # Pick ONE token by RANDOMLY SAMPLING from the probability distribution.\n",
    "            # Higher probability = more likely to be picked. But NOT guaranteed.\n",
    "            #\n",
    "            # This is different from just taking the HIGHEST probability (argmax).\n",
    "            # Sampling keeps the output VARIED and interesting.\n",
    "            # Argmax always picks the same token ‚Üí boring, repetitive text.\n",
    "            #\n",
    "            # Example with probs = [.22, .30, .18, .27]:\n",
    "            #   'b' has 30% chance of being picked\n",
    "            #   'a' has 22% chance\n",
    "            #   'd' has 27% chance\n",
    "            #   'c' has 18% chance\n",
    "            #   ‚Üí maybe this roll picks 'd' ‚Üí idx_next = [[3]]\n",
    "            #\n",
    "            # idx_next shape: (B, 1) = (1, 1) ‚Üí one new token per sequence\n",
    "\n",
    "            # ‚îÄ‚îÄ STEP 5: APPEND NEW TOKEN TO THE SEQUENCE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            # Glue the new token onto the END of the current sequence.\n",
    "            # dim=1 means \"concatenate along the T dimension\" (add a new column).\n",
    "            #\n",
    "            # Example (iteration by iteration):\n",
    "            #\n",
    "            #   Start:       idx = [[0]]           shape: (1, 1)  ‚Üí 'a'\n",
    "            #   After loop1: idx = [[0, 3]]         shape: (1, 2)  ‚Üí 'a','d'\n",
    "            #   After loop2: idx = [[0, 3, 1]]      shape: (1, 3)  ‚Üí 'a','d','b'\n",
    "            #   After loop3: idx = [[0, 3, 1, 2]]   shape: (1, 4)  ‚Üí 'a','d','b','c'\n",
    "            #   ...and so on for max_new_tokens steps\n",
    "            #\n",
    "            # Each loop, T grows by 1. After 100 loops, T = original_T + 100.\n",
    "\n",
    "        return idx\n",
    "        # Returns the FULL sequence: original seed tokens + all newly generated tokens.\n",
    "        # Shape: (B, T + max_new_tokens) = (1, 1 + 100) = (1, 101) in our example.\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ OUTSIDE THE CLASS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "# BUILD the model. Runs __init__ once.\n",
    "# Lookup table is created with random starting numbers.\n",
    "# Like: \"Build the kitchen and put in a blank (random) cheat sheet.\"\n",
    "# Nothing is learned yet. Just the structure is ready.\n",
    "\n",
    "logits, loss = m(xb, yb)\n",
    "# RUN the model on training data. Returns predictions + loss.\n",
    "#\n",
    "#   m = BigramLanguageModel(vocab_size) ‚Üí BUILD  (set up the kitchen)\n",
    "#   logits, loss = m(xb, yb)           ‚Üí RUN    (cook and taste the food)\n",
    "#                  ‚Üë       ‚Üë\n",
    "#              predictions  how wrong we are\n",
    "\n",
    "print(\"The shape of the logits is\", logits.shape)\n",
    "# torch.Size([6, 4]) = (B*T, C) = 6 predictions, 4 scores each ‚úÖ\n",
    "\n",
    "print(\"The loss is:\", loss)\n",
    "# tensor(‚âà1.38) ‚Üê close to log(4) = 1.386, random model as expected ‚úÖ\n",
    "\n",
    "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "# This line does 5 things chained together. Reading inside ‚Üí out:\n",
    "#\n",
    "# ‚îÄ‚îÄ PIECE 1: torch.zeros((1, 1), dtype=torch.long) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Creates the STARTING SEED ‚Äî a (1√ó1) tensor containing just [[0]].\n",
    "# Token id=0 is used as the \"kickoff\" seed ‚Äî like pressing Enter to start.\n",
    "#\n",
    "#   ‚îå‚îÄ‚îÄ‚îÄ‚îê\n",
    "#   ‚îÇ 0 ‚îÇ  ‚Üê token id=0 = 'a' in our vocab\n",
    "#   ‚îî‚îÄ‚îÄ‚îÄ‚îò\n",
    "#   shape: (B=1, T=1)\n",
    "#   B=1 = 1 sequence EXISTS in the batch (COUNT, not index!)\n",
    "#   T=1 = that sequence is 1 token long\n",
    "#\n",
    "# ‚îÄ‚îÄ PIECE 2: m.generate(..., max_new_tokens=100) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Runs the generate loop 100 times.\n",
    "# Each loop adds one new token to the sequence.\n",
    "#\n",
    "#   Seed idx = [[0]]   shape: (1, 1)  ‚Üí just 'a'\n",
    "#   Loop 1: last token='a' ‚Üí samples 'd' ‚Üí idx=[[0,3]]\n",
    "#   Loop 2: last token='d' ‚Üí samples 'b' ‚Üí idx=[[0,3,1]]\n",
    "#   Loop 3: last token='b' ‚Üí samples 'c' ‚Üí idx=[[0,3,1,2]]\n",
    "#   ... 100 times total\n",
    "#\n",
    "#   Returns: [[0, 3, 1, 2, ...]]   shape: (B=1, T=101)\n",
    "#\n",
    "# ‚îÄ‚îÄ PIECE 3: [0] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Grabs the FIRST (and only) sequence from the batch.\n",
    "# Shape goes from (1, 101) ‚Üí (101,) ‚Üí just a flat list of 101 token IDs.\n",
    "#\n",
    "#   [[0, 3, 1, 2, ...]][0]  ‚Üí  [0, 3, 1, 2, ...]\n",
    "#\n",
    "# WHY [0] and not [1]?\n",
    "# B=1 means 1 sequence EXISTS (the COUNT).\n",
    "# [0] is how we ACCESS it (the INDEX). Indexing always starts at 0.\n",
    "# B=1 ‚Üí only valid index is [0].\n",
    "# B=3 ‚Üí valid indices would be [0], [1], [2].\n",
    "#\n",
    "#   B=1 (1 sequence in batch)       ‚Üê COUNT\n",
    "#        ‚Üì\n",
    "#   [[0, 3, 1, 2, ...]]  shape:(1,101)  ‚Üê full tensor\n",
    "#     ‚Üë\n",
    "#    [0]                              ‚Üê INDEX to grab the first sequence\n",
    "#        ‚Üì\n",
    "#   [0, 3, 1, 2, ...]    shape:(101,)  ‚Üê flat list, batch wrapper removed\n",
    "#\n",
    "# ‚îÄ‚îÄ PIECE 4: .tolist() ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Converts PyTorch tensor ‚Üí plain Python list.\n",
    "# e.g. tensor([0, 3, 1]) ‚Üí [0, 3, 1]\n",
    "# Needed because decode() expects a Python list, not a tensor.\n",
    "#\n",
    "# ‚îÄ‚îÄ PIECE 5: decode(...) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Converts token IDs back to readable characters.\n",
    "# e.g. [0, 3, 1, 2] ‚Üí \"adbc\"\n",
    "#\n",
    "# ‚îÄ‚îÄ FULL CHAIN IN ONE PICTURE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "#\n",
    "#  torch.zeros  ‚Üí m.generate()  ‚Üí    [0]      ‚Üí .tolist() ‚Üí decode() ‚Üí print\n",
    "#      ‚Üì               ‚Üì               ‚Üì            ‚Üì          ‚Üì\n",
    "#   [[0]]       [[0,3,1,2...]]   [0,3,1,2...]  [0,3,1,2...]  \"adbc...\"\n",
    "#  (1,1)           (1,101)          (101,)      Python list   text! üéâ\n",
    "#  seed token   full sequence    1 sequence     no tensor\n",
    "#\n",
    "# QUESTION: \"Are we feeding the entire history or context?\"\n",
    "# Technically YES ‚Äî we feed the full growing idx to forward() each time.\n",
    "# BUT the Bigram model throws away everything EXCEPT the last token!\n",
    "# (That's what logits[:, -1, :] does ‚Äî it ignores all but the final position.)\n",
    "# So in practice, Bigram has NO memory. It only ever looks at 1 token back.\n",
    "# This is its biggest weakness ‚Äî and exactly why we'll need Transformers later!\n",
    "#\n",
    "#   GPT/Transformer: \"I look at ALL previous tokens to decide what's next\"\n",
    "#   Bigram:          \"I only look at the LAST token. History? What history?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# WHAT IS AN OPTIMIZER?\n",
    "# Think of your neural network like a person lost in hilly \n",
    "# terrain (the \"loss landscape\"), trying to find the lowest \n",
    "# valley (lowest loss). The optimizer is their strategy for \n",
    "# walking downhill.\n",
    "# =============================================================\n",
    "\n",
    "# AdamW = \"Adam\" optimizer + \"Weight Decay\" fix\n",
    "# 'm.parameters()' = we hand AdamW ALL the knobs (weights) \n",
    "#                    inside our model that it's allowed to tune\n",
    "# lr = \"learning rate\" = how BIG each step is when walking downhill\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "# lr = 1e-3 means 0.001\n",
    "# Why 0.001?\n",
    "#   - Too HIGH (e.g. 0.1)  ‚Üí you overshoot the valley, bounce around, never settle\n",
    "#   - Too LOW  (e.g. 1e-7) ‚Üí you move SO slowly, training takes forever\n",
    "#   - 1e-3 is a \"safe default\" that works well for Adam-based optimizers\n",
    "#   - Karpathy uses 1e-3 here because this is a small, toy-scale model\n",
    "#   - For bigger models (like real GPT), lr is often ~3e-4 with a scheduler\n",
    "\n",
    "\n",
    "## How Does Adam Work? (Intuitively)\n",
    "\n",
    "#Think of training like rolling a ball down a hilly landscape to find the lowest point.\n",
    "\n",
    "#**Plain SGD** gives the ball a push in the downhill direction. That's it. Same size push every time.\n",
    "\n",
    "#**Adam** is much smarter. It does **two extra things** on top of SGD:\n",
    "\n",
    "#---\n",
    "\n",
    "### üß≠ Thing 1: It remembers *direction* (Momentum)\n",
    "#> \"Which way have I *mostly* been going recently?\"\n",
    "\n",
    "#Adam keeps a **running average of past gradients** (directions). If you've been consistently moving left, it builds up speed in that direction ‚Äî like a ball gaining momentum rolling downhill.\n",
    "\n",
    "#This helps it **not get confused by noisy, jumpy gradients**.\n",
    "\n",
    "#---\n",
    "\n",
    "### üìè Thing 2: It adjusts *step size per weight* (Adaptive Learning Rate)\n",
    "#> \"How bumpy is this particular direction?\"\n",
    "\n",
    "#Adam also tracks **how large the gradients have been** for each individual weight. If one weight keeps getting huge gradients, Adam says *\"slow down here, it's bumpy\"*. If another weight gets tiny gradients, Adam says *\"speed up here, it's flat\"*.\n",
    "\n",
    "#This means **every single weight gets its own personal learning rate**, automatically.\n",
    "\n",
    "\n",
    "### The Formula (Simply Put):\n",
    "\n",
    "### new_weight = old_weight - lr √ó (momentum / (sqrt(squared_avg) + tiny_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5473523139953613\n"
     ]
    }
   ],
   "source": [
    "## The Big Picture: One Loop Iteration\n",
    "\n",
    "#Here's the **full cycle** every single step:\n",
    "#\n",
    "#‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "#‚îÇ                                                     ‚îÇ\n",
    "#‚îÇ  1. get_batch()  ‚Üí  grab 32 random training chunks  ‚îÇ\n",
    "#‚îÇ         ‚Üì                                           ‚îÇ\n",
    "#‚îÇ  2. m(xb, yb)    ‚Üí  model predicts, measures loss   ‚îÇ\n",
    "#‚îÇ         ‚Üì                                           ‚îÇ\n",
    "#‚îÇ  3. zero_grad()  ‚Üí  wipe the slate clean            ‚îÇ\n",
    "#‚îÇ         ‚Üì                                           ‚îÇ\n",
    "#‚îÇ  4. loss.backward() ‚Üí figure out WHO caused the loss‚îÇ\n",
    "#‚îÇ         ‚Üì                                           ‚îÇ\n",
    "#‚îÇ  5. optimizer.step() ‚Üí nudge all weights to improve ‚îÇ\n",
    "#‚îÇ                                                     ‚îÇ\n",
    "#‚îÇ  Repeat 1000x ‚Üí model gets smarter each time üß†     ‚îÇ\n",
    "#‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "# THE TRAINING LOOP ‚Äî This is the HEART of learning.\n",
    "# Every iteration = one \"experience\" the model learns from.\n",
    "# Think of it like a student doing 1000 practice problems.\n",
    "# =============================================================\n",
    "\n",
    "# batch_size = 32 means: \n",
    "#   \"Don't learn from 1 example at a time ‚Äî grab 32 examples \n",
    "#    simultaneously and learn from all of them at once\"\n",
    "# Why 32 and not 1? Or 10,000?\n",
    "#   - Too small (1): very noisy signal, slow, unstable learning\n",
    "#   - Too large (10000): very smooth but needs huge memory & can get stuck\n",
    "#   - 32 is a sweet spot: stable signal, fits in memory, fast\n",
    "batch_size = 32\n",
    "\n",
    "# Run 1000 training steps (1000 practice rounds)\n",
    "for steps in range(20000):\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # STEP 1: GET A BATCH OF DATA\n",
    "    # get_batch() is defined earlier in nanoGPT ‚Äî it randomly \n",
    "    # grabs 'batch_size' chunks from the training text.\n",
    "    # \n",
    "    # xb = INPUT  tokens (e.g. \"The cat sat on\")   shape: [32, block_size]\n",
    "    # yb = TARGET tokens (e.g. \"cat sat on the\")   shape: [32, block_size]\n",
    "    #      yb is xb shifted by 1 position ‚Äî the \"correct answers\"\n",
    "    #\n",
    "    # Each row is an independent training example.\n",
    "    # We get 32 of them at once = one batch.\n",
    "    # ----------------------------------------------------------\n",
    "    xb, yb = get_batch('train')  # 'train' = use training data, not validation\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # STEP 2: FORWARD PASS ‚Äî Run the model, compute the loss\n",
    "    # The model looks at xb, makes predictions (logits),\n",
    "    # then compares predictions to yb (the right answers).\n",
    "    # 'loss' is a single number: how WRONG the model is right now.\n",
    "    # Lower loss = better predictions.\n",
    "    # ----------------------------------------------------------\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # STEP 3: ZERO OUT OLD GRADIENTS (CRITICAL ‚Äî easy to forget!)\n",
    "    #\n",
    "    # Gradients are the \"feedback signals\" that tell each weight \n",
    "    # which direction to move. \n",
    "    #\n",
    "    # By DEFAULT, PyTorch ACCUMULATES (adds up) gradients across \n",
    "    # steps. If you don't clear them, the feedback from step 1 \n",
    "    # bleeds into step 2, step 3, etc. ‚Äî like trying to hear \n",
    "    # new music while the last song is still playing loudly.\n",
    "    #\n",
    "    # set_to_none=True ‚Üí instead of setting gradients to 0,\n",
    "    # it sets them to None (slightly faster & less memory)\n",
    "    # ----------------------------------------------------------\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # STEP 4: BACKWARD PASS ‚Äî Compute the gradients\n",
    "    # \n",
    "    # loss.backward() is where the MAGIC happens.\n",
    "    # PyTorch walks BACKWARDS through every operation in the model\n",
    "    # and asks: \"How much did each weight CONTRIBUTE to this loss?\"\n",
    "    # \n",
    "    # This uses the Chain Rule from calculus (backpropagation).\n",
    "    # The result: every weight now has a .grad value attached to it.\n",
    "    # \n",
    "    # Think of it as: \"Who is to blame for this mistake, and HOW MUCH?\"\n",
    "    # ----------------------------------------------------------\n",
    "    loss.backward()\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # STEP 5: UPDATE THE WEIGHTS ‚Äî The actual \"learning\" step\n",
    "    #\n",
    "    # Now that we know the gradient (direction of blame) for each \n",
    "    # weight, AdamW uses that info to NUDGE each weight in the \n",
    "    # direction that reduces loss.\n",
    "    #\n",
    "    # This is where AdamW's smart per-weight step sizes kick in.\n",
    "    # \n",
    "    # After this line, the model is SLIGHTLY smarter than before.\n",
    "    # Do this 1000 times ‚Üí the model has learned a lot.\n",
    "    # ----------------------------------------------------------\n",
    "    optimizer.step()\n",
    "\n",
    "# After all 1000 steps, print the final loss.\n",
    "# .item() converts the PyTorch tensor to a plain Python number.\n",
    "# If this number went DOWN from where it started ‚Üí learning worked! üéâ\n",
    "print(loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANoamery,\n",
      "\n",
      "Poove. f howendofld:\n",
      "BRCar wr uny fapou th len sthed chithuterist gin me.\n",
      "AReidy imut bearg, hendinsouto I ty,\n",
      "TIZAnghe ICHengouprearsonosomithizewile\n",
      "YCak inecor qurofous;\n",
      "Thole wo nthis myoaity\n",
      "ICHothad wror, he DUK:\n",
      "\n",
      "h s t;\n",
      "\n",
      "Se hen'To begh gang weepin pr heslooul w iounguare nche he bln gin, itl, str my y gue. cheinot y on hand\n",
      "\n",
      "u w t wiene, mollathevevie, bare, eat ule ue:\n",
      "INI t ful yountoteeagou blit tong chadef thisorecth?\n",
      "\n",
      "Aspeas agn st n ICOfttold hag-sttersind olldaitowee for bura-y'd d bous thiorlues;\n",
      "KI det, bus\n",
      "\n",
      "\n",
      "QUKI k ok;\n",
      "RCHe,\n",
      "I athy ss at byo, preror.\n",
      "Rid beres y\n",
      "V:\n",
      "MAnd,\n",
      "BONG sas ondessely, I thrichat ouprr IOLOLOLABun.\n",
      "Age:\n",
      "S:\n",
      "LIUCUS:\n",
      "\n",
      "OFFORETRARI pesu hond;\n",
      "ABy qungas t s\n",
      "Cay ghe:\n",
      "\n",
      "Y:\n",
      "\n",
      "Cro thoundeir t wit ter t, band ty, wewat befat tirgarsur tosee t, aththithayo thy mave a tullelir il ur sothardowhe lot rr lly 's o.\n",
      "LAmp:\n",
      "THAs cate,\n",
      "PELEN siter myoot me wit CERY:\n",
      "T: aveesher t alld cos nd fllled cak gry s dicowere\n",
      "Foth rppasos, fof frtou ayomane whethat'd\n"
     ]
    }
   ],
   "source": [
    "### Now that we have the loss to around ~2.547 Let's check the output\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Attention\n",
    "\n",
    "Core mechanism that allows tokens to communicate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement self-attention mechanism\n",
    "# - Queries, Keys, Values\n",
    "# - Attention scores and masking\n",
    "# - Weighted aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention\n",
    "\n",
    "Multiple attention heads running in parallel to attend to different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement multi-head attention\n",
    "# - Multiple heads\n",
    "# - Concatenation and projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Blocks\n",
    "\n",
    "Complete transformer block with:\n",
    "- Multi-head attention\n",
    "- Feed-forward network\n",
    "- Layer normalization\n",
    "- Residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement transformer block\n",
    "# - Attention sublayer\n",
    "# - FFN sublayer\n",
    "# - LayerNorm and residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full GPT Model\n",
    "\n",
    "Stack multiple transformer blocks and add token + position embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement full GPT model\n",
    "# - Token embeddings\n",
    "# - Position embeddings\n",
    "# - Stack of transformer blocks\n",
    "# - Final layer norm and linear head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# - Batch generation\n",
    "# - Forward pass and loss calculation\n",
    "# - Backward pass and optimization\n",
    "# - Logging and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generation\n",
    "\n",
    "Sample from the trained model to generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation\n",
    "# - Autoregressive sampling\n",
    "# - Temperature control\n",
    "# - Generate and decode samples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
